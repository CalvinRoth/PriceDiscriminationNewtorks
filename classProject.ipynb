{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dad66d-1244-4a97-8edb-0f2b37a325f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import numpy.linalg as lin\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import scipy.sparse.linalg as slin\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84362ea-fcb0-4b91-bb13-9382c1613d2b",
   "metadata": {},
   "source": [
    "#  Basic Functionality \n",
    "\n",
    "Here we supply the essential functions needed for experiments. This includes the ability to find the term relating to the spectral norm which is abstracted in a function because it may change. \n",
    "\n",
    "Also included a generators for ER graphs, the ability to compute the optimal price vector for a graph and the optimal profit of a graph if we were to apply this price vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2e3ca-273f-4407-8cb0-16647ae9803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear algebra\n",
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=np.inf)\n",
    "    # return np.sqrt(slin.eigs(A.T @ A, k=1, which=\"LM\", return_eigenvectors=False, tol=1e-10)[0])\n",
    "\n",
    "\n",
    "# Graph makers\n",
    "def makeSimilarGraph(G: nx.DiGraph) -> np.matrix:\n",
    "    \"\"\" Generates the new graph with the same in/out degree as the orginal\n",
    "        -------\n",
    "        Return adj. matrix of graph\n",
    "    \"\"\"\n",
    "    sequence_in = [d for _, d in G.in_degree]\n",
    "    sequence_out = [d for _, d in G.out_degree]\n",
    "    return nx.to_numpy_matrix(\n",
    "        nx.directed_configuration_model(sequence_in, sequence_out, create_using=nx.DiGraph),\n",
    "        dtype=\"d\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Graph Generators\n",
    "def makeERGraph(n: int, p: float) -> np.matrix:\n",
    "    \"\"\" Generates Random Erdos-Renyi Graphs with n vertices and link probability p\n",
    "        ------\n",
    "        return Adjacency graph of matrix and the networkx DiGraph object\n",
    "    \"\"\"\n",
    "    G = nx.generators.fast_gnp_random_graph(n, p, directed=True)\n",
    "    # sortG = sorted(G.in_degree, key=lambda x: x[1], reverse=True)\n",
    "    return nx.to_numpy_matrix(G, dtype=\"d\"), G\n",
    "\n",
    "\n",
    "def centralty(A: np.matrix, rho: float, alpha,k) -> np.matrix:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np matrix\n",
    "    rho : network effect\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Centrality vector as described in paper\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "\n",
    "    ident = np.eye(n, n)\n",
    "    ones = np.ones((n, 1))\n",
    "    ApA = A + A.T\n",
    "    eig = specNorm(ApA)\n",
    "    alpha = rho / eig\n",
    "    central = lin.inv(ident - (alpha * ApA))\n",
    "    central = central @ ones  # Checked.  this > 0\n",
    "    return central\n",
    "\n",
    "\n",
    "# Paper related properties\n",
    "def applyPriceVector(A: np.matrix, v: np.matrix, rho: float, a: int | float, c: int | float) -> (float, bool):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : Graph\n",
    "    v : price vector\n",
    "    rho : network strength\n",
    "    a : Stand alone strength\n",
    "    c : Marginal cost. Should be less than a\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Profit in this network if prces v were applied.\n",
    "    And if result is valid or not\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    ident = np.eye(n, n)\n",
    "    ones = np.ones((n, 1))\n",
    "    ApA = A + A.T\n",
    "    # spN = specNorm(ApA)  # Sometimes scipy return x+0i, this is to discard warning\n",
    "    alpha = (rho / specNorm(ApA))\n",
    "    consumption = (2 * alpha) * A\n",
    "    consumption = ident - consumption\n",
    "    consumption = 0.5 * lin.inv(consumption)  # This is entirely in the range [0,1] ^ checked\n",
    "\n",
    "    consumption = consumption @ ((a * ones) - v)\n",
    "    valid = 1\n",
    "    if (np.min(consumption) < 0):\n",
    "        valid = 0\n",
    "    return ((v - (c * ones)).T @ consumption)[0, 0], valid\n",
    "\n",
    "\n",
    "def priceVector(A: np.matrix, rho: float, a: int | float, c: int | float,k = None) -> np.matrix:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : Network\n",
    "    rho : network strength\n",
    "    a : stand alone util\n",
    "    c : marginal cost. Should be less than a\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Vector reprsenting what price to charge individual i\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    ones = np.ones((n, 1))\n",
    "    alpha = rho / specNorm(A + A.T)\n",
    "    central = centralty(A, rho, alpha,k)  # This should be A not A + A.T because of how centralty function is designed\n",
    "    dif = A - A.T\n",
    "    pv1 = ((a + c) / 2) * ones\n",
    "    pv2 = ((a - c) * alpha * 0.5) * (dif @ central)\n",
    "    return pv1 + pv2\n",
    "\n",
    "\n",
    "def optimalProfit(A: np.matrix, n: int, a: int | float, c: int | float, rho: float):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : Network\n",
    "    n : size of network\n",
    "    rho : network strength\n",
    "    a : stand alone util\n",
    "    c : marginal cost. Should be less than a\n",
    "    Returns\n",
    "    -------\n",
    "    True profit. Should be the same as applyPriceVector(A, pricevector(A,...),...)\n",
    "    \"\"\"\n",
    "    one = np.ones((n, 1))\n",
    "    alpha = rho / specNorm(A + A.T)\n",
    "\n",
    "    t1 = lin.inv(np.eye(n, n) - (alpha * (A + A.T)))\n",
    "    total = one.T @ t1 @ one\n",
    "    total = ((a - c) * (a - c) / 8) * total\n",
    "    return np.real(total[0, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a031713-f69d-4769-b2c5-842d0977bdec",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "Here are functions used \"one level up\" in terms of abstraction from finding the price vectors. This includes things like computing the fractional regret of a applying a given vector v to a graph instad of its true optimal price vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f8566-198e-4580-b78e-092575f7c137",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def fractionalRegret(A, v, n, rho, a, c):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : Network\n",
    "    v : price vector to compare to\n",
    "    rho : network strength\n",
    "    n : number of nodes\n",
    "    a : stand alone util\n",
    "    c : marginal cost. Should be less than a\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    1 - (profit of A using v)/(profit of A using best choice)\n",
    "    \"\"\"\n",
    "    discrim = optimalProfit(A, n, a, c, rho)  # Optimal profit\n",
    "    # I have check and the formula for optimal profit does match applypricevector(A, pricevector(A,...), params)\n",
    "    appliedProf = applyPriceVector(A, v, rho, a, c)  # Profit at v\n",
    "    return 1 - (appliedProf / discrim)\n",
    "\n",
    "\n",
    "\n",
    "# Gaps applying price vector of guesses to true graph G\n",
    "def getGapRev(A, test, rho, a, c):\n",
    "    \"\"\" Apply optimal profit price vector guess graph to test graph test and A. Return pair of profits\"\"\"\n",
    "    optimalVector = priceVector(test, rho, a, c)\n",
    "    profitWithGuessV = applyPriceVector(A, optimalVector, rho, a, c)\n",
    "    trueProfit = applyPriceVector(A, priceVector(A, rho, a, c), rho, a, c)\n",
    "    return trueProfit, profitWithGuessV\n",
    "\n",
    "\n",
    "# Applying the true optimal profit vector to guesses\n",
    "# Currently not using because it seems backwards of what I want\n",
    "def getGaps(n, p, rho, a, c, i, results, n_trials):\n",
    "    A, G = makeERGraph(n, p)\n",
    "    results[i] = np.average([getGap(A, makeSimilarGraph(G), rho, a, c) for j in range(n_trials)])\n",
    "    return i\n",
    "\n",
    "\n",
    "# Apply the price vector that each guess produces to the true graph and take average.\n",
    "def getGapsReverse(n, p, rho, a, c, i, results, n_trials):\n",
    "    A, G = makeERGraph(n, p)\n",
    "    results[i] = np.average([getGapRev(A, makeSimilarGraph(G), rho, a, c) for j in range(n_trials)])\n",
    "    return i\n",
    "\n",
    "\n",
    "# Here we apply the average optimal price vector of the guesses to the true graph G\n",
    "# I get a warning about discarding complex values, values should never complex for this problem\n",
    "# and when I check they are all +0i so ?\n",
    "\n",
    "\n",
    "def getAverageGap(n, p, rho, a, c, i, results, n_trials):\n",
    "    A, G = makeERGraph(n, p)\n",
    "    n = A.shape[0]\n",
    "    trueProfit = applyPriceVector(A, priceVector(A, rho, a, c), rho, a, c)\n",
    "    # the average vector initilized with sample size of 1\n",
    "    averageV = priceVector(makeSimilarGraph(G), rho, a, c)\n",
    "    # And another n_trials-1 trials\n",
    "    for j in range(n_trials - 1):\n",
    "        averageV += priceVector(makeSimilarGraph(G), rho, a, c)\n",
    "    averageV /= n_trials  # Scaling\n",
    "    profit = applyPriceVector(A, averageV, rho, a, c)\n",
    "    results[i] = np.real(trueProfit - profit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f1b70-3ce4-4d09-bf24-50f6194a4ebe",
   "metadata": {},
   "source": [
    "# Tests\n",
    "Here are the collection of various tests we can run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba8bcfa-e504-47af-bda9-bec31f4b432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profitDistribution(n : int, p : float,  n_trials : int, rho : float, a : int | float ,c : int | float):\n",
    "    results = np.zeros(n_trials)\n",
    "    A_true, G_true = makeERGraph(n, p)\n",
    "    trueP = optimalProfit(A_true,n, a, c, rho)\n",
    "    res_par = np.zeros((n_trials,))\n",
    "    res_seq = np.zeros((n_trials,))\n",
    "    valid_par = 0\n",
    "    valid_seq = 0    \n",
    "    for j in range(n_trials):\n",
    "        A_par, G_par = makeERGraph(n,p)\n",
    "        A_seq = makeSimilarGraph(G_true)\n",
    "        v_par = priceVector(A_par, rho, a, c)\n",
    "        v_seq = priceVector(A_seq, rho, a, c)\n",
    "        res_par[j],b0 = applyPriceVector(A_true, v_par, rho, a, c)\n",
    "        res_seq[j],b1 = applyPriceVector(A_true, v_seq, rho, a, c)\n",
    "        valid_par += b0\n",
    "        valid_seq += b1\n",
    "        \n",
    "    return [res_par, res_seq], [valid_par/n_trials, valid_seq/n_trials]\n",
    "\n",
    "\n",
    "def regretDistribution(n : int, p : float,  n_trials : int, rho : float, a : int | float ,c : int | float):\n",
    "    results = np.zeros(n_trials)\n",
    "    A_true, G_true = makeERGraph(n, p)\n",
    "    trueP = optimalProfit(A_true,n, a, c, rho)\n",
    "    res_par = np.zeros((n_trials,))\n",
    "    res_seq = np.zeros((n_trials,))\n",
    "    valid_par = 0\n",
    "    valid_seq = 0    \n",
    "    for j in range(n_trials):\n",
    "        A_par, G_par = makeERGraph(n,p)\n",
    "        A_seq = makeSimilarGraph(G_true)\n",
    "        v_par = priceVector(A_par, rho, a, c)\n",
    "        v_seq = priceVector(A_seq, rho, a, c)\n",
    "        res_par[j],b0 = fractionalRegret(A_true, v_par, n, rho, a, c)\n",
    "        res_seq[j],b1 = fractionalRegret(A_true, v_seq, n, rho, a, c)\n",
    "        valid_par += b0\n",
    "        valid_seq += b1\n",
    "        \n",
    "    return [res_par, res_seq], [valid_par/n_trials, valid_seq/n_trials]\n",
    "\n",
    "def walkDistribution(n : int, p : float,  n_trials : int, rho : float, a : int | float ,c : int | float,k : int):\n",
    "    results = np.zeros(n_trials)\n",
    "    res = np.zeros((n_trials,))\n",
    "    for j in range(n_trials):\n",
    "        A_true, G_true = makeERGraph(n, p)\n",
    "        trueP = optimalProfit(A_true,n, a, c, rho)\n",
    "        v = priceVector(A_true, rho, a, c,k)\n",
    "        res[j] = fractionalRegret(A_true, v, n, rho, a, c)[0]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of different Norms\n",
    "Reporting for n=1500, rho=0.9, p=sqrt(log(n))/n, a=5, c=4 the variance of profits and the fraction of valid samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinity Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93229087-9fb1-480c-b831-35ed3fa539e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinity Norm\n",
      "Same Parameter var 6.806609044439233\n",
      "Same Sequence var 0.0012618488411177784\n",
      "Same Parameter mean 256.5302924592616\n",
      "Same Sequence mean 274.2245017496105\n",
      "Same Parameter ratio 1.0\n",
      "Same Sequence ratio 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=np.inf)\n",
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "[res_par, res_seq], [ratio_par, ratio_seq] = profitDistribution(n,p,n_trials,  rho, a, c)\n",
    "[regret_par, regret_seq] = regretDistribution(n,p,n_trials,  rho, a, c)\n",
    "print(\"Infinity Norm\")\n",
    "print(\"Same Parameter var\", np.var(res_par)), print(\"Same Sequence var\", np.var(res_seq))\n",
    "print(\"Same Parameter mean\", np.mean(res_par)), print(\"Same Sequence mean\", np.mean(res_seq))\n",
    "print(\"Same Parameter ratio\", ratio_par), print(\"Same Sequence ratio\", ratio_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frobenieus Norm\n",
    "It appears this norm is 2 large so the network effect is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407e584e-b74f-4b67-a4ad-48c04c962258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A)\n",
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "[res_par, res_seq], [ratio_par, ratio_seq] = profitDistribution(n,p,n_trials,  rho, a, c)\n",
    "[regret_par, regret_seq] = regretDistribution(n,p,n_trials,  rho, a, c)\n",
    "print(\"Infinity Norm\")\n",
    "print(\"Same Parameter var\", np.var(res_par)), print(\"Same Sequence var\", np.var(res_seq))\n",
    "print(\"Same Parameter mean\", np.mean(res_par)), print(\"Same Sequence mean\", np.mean(res_seq))\n",
    "print(\"Same Parameter ratio\", ratio_par), print(\"Same Sequence ratio\", ratio_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=1)\n",
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "[res_par, res_seq], [ratio_par, ratio_seq] = profitDistribution(n,p,n_trials,  rho, a, c)\n",
    "[regret_par, regret_seq] = regretDistribution(n,p,n_trials,  rho, a, c)\n",
    "print(\"One Norm\")\n",
    "print(\"Same Parameter var\", np.var(res_par)), print(\"Same Sequence var\", np.var(res_seq))\n",
    "print(\"Same Parameter mean\", np.mean(res_par)), print(\"Same Sequence mean\", np.mean(res_seq))\n",
    "print(\"Same Parameter ratio\", ratio_par), print(\"Same Sequence ratio\", ratio_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuclear norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=\"nuc\")\n",
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "#[res_par, res_seq], [ratio_par, ratio_seq] = profitDistribution(n,p,n_trials,  rho, a, c)\n",
    "#[regret_par, regret_seq] = regretDistribution(n,p,n_trials,  rho, a, c)\n",
    "#print(\"One Norm\")\n",
    "#print(\"Same Parameter var\", np.var(res_par)), print(\"Same Sequence var\", np.var(res_seq))\n",
    "#print(\"Same Parameter mean\", np.mean(res_par)), print(\"Same Sequence mean\", np.mean(res_seq))\n",
    "#print(\"Same Parameter ratio\", ratio_par), print(\"Same Sequence ratio\", ratio_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Norm\n",
    "This is very confusing and worrying. The results show that under the 2 norm same parameter graphs are likely to be malformed often. Also this code takes much longer to run than the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=2)\n",
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "\"\"\"\n",
    "[res_par, res_seq], [ratio_par, ratio_seq] = profitDistribution(n,p,n_trials,  rho, a, c)\n",
    "[regret_par, regret_seq] = regretDistribution(n,p,n_trials,  rho, a, c)\n",
    "print(\"2 Norm\")\n",
    "print(\"Same Parameter var\", np.var(res_par)), print(\"Same Sequence var\", np.var(res_seq))\n",
    "print(\"Same Parameter mean\", np.mean(res_par)), print(\"Same Sequence mean\", np.mean(res_seq))\n",
    "print(\"Same Parameter ratio\", ratio_par), print(\"Same Sequence ratio\", ratio_seq)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that seemingly any choice of norm besides 2-norm is sufficient. It is unclear to me if I need to weight these norms in someway to be bigger/smamer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limited Walk Size\n",
    "One question we have asked is if knowing the degrees of each matrix is sufficient perhaps knowing the reach of each node in two steps is better because it specifies the graph more. Here instead of generating new graphs specificied by this information we will look at the regret using only the true graph but with truncated centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specNorm(A: np.matrix) -> float:\n",
    "    return lin.norm(A, ord=np.inf)\n",
    "def centralty(A: np.matrix, rho: float, alpha, k=None) -> np.matrix:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : np matrix\n",
    "    rho : network effect\n",
    "    k : size of walk to take in network\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Centrality vector as described in paper\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "\n",
    "    if k is None:\n",
    "\n",
    "        ident = np.eye(n, n)\n",
    "        ones = np.ones((n, 1))\n",
    "        ApA = A + A.T\n",
    "        eig = specNorm(ApA)\n",
    "        alpha = rho / eig\n",
    "        central = lin.inv(ident - (alpha * ApA))\n",
    "        central = central @ ones  # Checked.  this > 0\n",
    "        return central\n",
    "    else:\n",
    "        one = np.ones((n,1))\n",
    "        ApA = A + A.T\n",
    "        alpha = rho / specNorm(ApA)\n",
    "        total = alpha * ApA\n",
    "        base = alpha * np.eye(n, n)\n",
    "        for i in range(1,k+1):\n",
    "            total = total + np.linalg.matrix_power(base, i)\n",
    "        return total @ one   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "p = np.sqrt(np.log(n))/n\n",
    "n_trials = 50\n",
    "rho = 0.9\n",
    "a = 5\n",
    "c = 4\n",
    "k = 1\n",
    "v = walkDistribution(n, p, n_trials, rho, a, c,k)\n",
    "print(\"k is \", k)\n",
    "print(\"Mean Loss\", np.mean(v))\n",
    "print(\"Variance\", np.var(v))\n",
    "k = 2 \n",
    "v = walkDistribution(n, p, n_trials, rho, a, c,k)\n",
    "print(\"k is \", k)\n",
    "print(\"Mean Loss\", np.mean(v))\n",
    "print(\"Variance\", np.var(v))\n",
    "k = n\n",
    "v = walkDistribution(n, p, n_trials, rho, a, c,k)\n",
    "print(\"k is \", k)\n",
    "print(\"Mean Loss\", np.mean(v))  \n",
    "print(\"Variance\", np.var(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8bf12b52825272ca753cd984c5229bfdc42f0a289f2d02e86b988d3513b4f71"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pDL': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
